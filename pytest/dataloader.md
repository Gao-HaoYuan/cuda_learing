1) Datasetï¼šè¿”å›å¼ é‡ + è½»é‡ metaï¼ˆä¸è¦æ”¾æ•´æ®µéŸ³é¢‘ï¼‰
    ```python
    class DNSDataset(data.Dataset):
        dataset_name = "DNS"

        def __init__(self, noise_list, clean_list):
            super().__init__()
            self.noise_list = noise_list
            half = len(clean_list) // 2
            self.clean_list = clean_list[:half]

        def __len__(self):
            return len(self.clean_list)

        def __getitem__(self, item):
            scale = max(random.uniform(0, 1), 0.05)
            snr = random.choice([-5, 0, 5, 10, 15])

            clean_path = self.clean_list[item]
            clean = sf.read(clean_path, dtype="float32")[0]

            noise_path = random.choice(self.noise_list)
            noise = sf.read(noise_path, dtype="float32")[0]

            noisy, clean, clean_rms = mix_data(clean, noise, snr, scale)

            # å¼ é‡ï¼ˆä¼šè¢«å †å ï¼‰
            noisy = torch.as_tensor(noisy, dtype=torch.float32)
            clean = torch.as_tensor(clean, dtype=torch.float32)
            clean_rms = torch.as_tensor(clean_rms, dtype=torch.float32)

            # è½»é‡å…ƒæ•°æ®ï¼ˆä»…æ ‡é‡/å­—ç¬¦ä¸²ï¼‰
            meta = dict(
                idx=item,
                snr=snr,
                scale=scale,
                clean_path=clean_path,
                noise_path=noise_path,
                clean_len=len(clean),
                noisy_len=len(noisy),
            )
            return noisy, clean, clean_rms, meta
    ```

2) è‡ªå®šä¹‰ collate_fnï¼šå¼ é‡å †å ï¼Œmeta å˜æˆ list
    ```python
    from torch.utils.data._utils.collate import default_collate

    def collate_keep_meta(batch):
        noisy, clean, clean_rms, meta = zip(*batch)   # é•¿åº¦ = batch_size
        return (
            default_collate(noisy),       # [B, T] æˆ– pad å [B, T_max]
            default_collate(clean),       # [B, T]
            default_collate(clean_rms),   # [B]
            list(meta),                   # ä¿æŒä¸º list[dict]ï¼Œä¸åšå †å 
        )
    ```


å¦‚æœéŸ³é¢‘é•¿åº¦ä¸ä¸€è‡´ï¼Œè¯·åœ¨ collate_fn é‡Œåš paddingï¼ˆè¿™é‡Œç•¥ï¼‰ã€‚å¦åˆ™ä¼šå‡ºç° â€œstack expects each tensor to be equal sizeâ€ã€‚

3) DataLoaderï¼šæŒ‡å®š collate_fn
    ```python
    train_loader = DataLoader(
        train_set,
        batch_size=batch_size,
        num_workers=num_workers,
        drop_last=True,
        pin_memory=True,
        sampler=train_sampler,
        collate_fn=collate_keep_meta,   # ğŸ‘ˆ å…³é”®
    )
    ```

4) è®­ç»ƒå¾ªç¯ï¼šæ ¹æ®â€œå¤–éƒ¨çš„æ¢¯åº¦èŒƒæ•°â€å†³å®šæ˜¯å¦æ‰“å° meta
    ```python
    for it, (noisy, clean, clean_rms, metas) in enumerate(train_loader):
        noisy = noisy.to(device, non_blocking=True)
        clean = clean.to(device, non_blocking=True)
        clean_rms = clean_rms.to(device, non_blocking=True)

        out = model(noisy)
        loss = criterion(out, clean)
        optimizer.zero_grad(set_to_none=True)
        loss.backward()

        # è®¡ç®—æ¢¯åº¦èŒƒæ•°ï¼ˆç¤ºä¾‹ï¼šå…¨å±€èŒƒæ•°ï¼‰
        total_norm = 0.0
        for p in model.parameters():
            if p.grad is not None:
                total_norm += p.grad.data.norm(2).item() ** 2
        total_norm = total_norm ** 0.5

        if total_norm > 100:  # å¤–éƒ¨æ¡ä»¶æˆç«‹ -> æ‰“å°/å†™æ–‡ä»¶
            print(f"[Grad={total_norm:.2f}] dumping metas for iter {it}:")
            for m in metas:   # metas æ˜¯ list[dict]
                print(f"  idx={m['idx']}, snr={m['snr']}, scale={m['scale']}, "
                    f"clean={m['clean_path']}, noise={m['noise_path']}")
            # æˆ–è€…å†™åˆ°æ–‡ä»¶
            # with open("hit.txt","a",encoding="utf-8") as f: ...

        optimizer.step()
    ```

è¿™æ ·ï¼š

Dataset é‡Œç”Ÿæˆçš„å‚æ•°ï¼ˆsnr/scale/è·¯å¾„/...ï¼‰æˆåŠŸå¸¦åˆ°å¤–éƒ¨ï¼›

æ˜¯å¦æ‰“å°å®Œå…¨ç”±å¤–éƒ¨æ§åˆ¶ï¼›

DataLoader æ­£å¸¸ï¼›meta ä¸å‚ä¸è®¡ç®—ï¼Œä¹Ÿä¸è¿› GPUï¼ˆåªæ˜¯ä¸ª listï¼‰ã€‚